# Iris Recognition System - H·ªá th·ªëng Nh·∫≠n di·ªán M·ªëng m·∫Øt

## üìã T·ªïng quan D·ª± √°n

D·ª± √°n x√¢y d·ª±ng m·ªôt h·ªá th·ªëng nh·∫≠n di·ªán m·ªëng m·∫Øt (Iris Recognition) end-to-end s·ª≠ d·ª•ng Deep Learning, bao g·ªìm 2 giai ƒëo·∫°n ch√≠nh:

1. **Segmentation**: Ph√¢n ƒëo·∫°n v√πng m·ªëng m·∫Øt t·ª´ ·∫£nh m·∫Øt
2. **Recognition**: Tr√≠ch xu·∫•t embedding v√† nh·∫≠n di·ªán danh t√≠nh

### Ki·∫øn tr√∫c T·ªïng th·ªÉ

```
Input Image ‚Üí Segmentation Model ‚Üí Iris Mask ‚Üí ROI Extraction ‚Üí Recognition Model ‚Üí Embedding ‚Üí Matching
```

---

## üèóÔ∏è Ki·∫øn tr√∫c H·ªá th·ªëng

### 1. **Segmentation Module** (`src/models/segmentation.py`)

#### Model: U-Net v·ªõi ResNet34 Encoder

- **Encoder**: ResNet34 pretrained tr√™n ImageNet
- **Decoder**: Upsampling blocks v·ªõi skip connections
- **Output**: Binary mask (224√ó224) - v√πng m·ªëng m·∫Øt vs background

**Ki·∫øn tr√∫c chi ti·∫øt:**

```python
UNetSegmentationModel(
    encoder: ResNet34 (pretrained)
    decoder: Sequential upsampling blocks
    input_size: (3, 224, 224)
    output_size: (1, 224, 224)
)
```

**ƒê·∫∑c ƒëi·ªÉm k·ªπ thu·∫≠t:**

- Input: RGB image (224√ó224√ó3)
- Output: Binary mask (224√ó224√ó1), gi√° tr·ªã [0, 1] sau sigmoid
- Pretrained encoder gi√∫p extract features t·ªët h∆°n
- Skip connections gi·ªØ th√¥ng tin spatial resolution

#### Loss Function: Binary Cross-Entropy with Logits

```python
criterion = nn.BCEWithLogitsLoss()
```

- √Åp d·ª•ng sigmoid v√† BCE loss trong 1 operation (numerically stable)
- Ph√π h·ª£p cho b√†i to√°n binary segmentation
- Loss = -[y*log(p) + (1-y)*log(1-p)]

### 2. **Recognition Module** (`src/models/recognition.py`)

#### Model: ResNet18-based Embedding Network

- **Backbone**: ResNet18 (c√≥ th·ªÉ pretrained)
- **Embedding head**: Fully connected layer
- **Output**: 128-dimensional L2-normalized embedding

**Ki·∫øn tr√∫c chi ti·∫øt:**

```python
RecognitionModel(
    backbone: ResNet18
    embedding_dim: 128
    num_classes: N (s·ªë ng∆∞·ªùi trong t·∫≠p train)
    embedding = backbone_features ‚Üí fc_embedding ‚Üí L2_normalize
)
```

**ƒê·∫∑c ƒëi·ªÉm k·ªπ thu·∫≠t:**

- Input: ROI image (3√ó224√ó224) - v√πng m·ªëng m·∫Øt ƒë√£ ƒë∆∞·ª£c crop v√† normalize
- Output: 128-dim embedding vector (L2-normalized)
- Embedding space: Cosine similarity metric

#### Loss Function: ArcFace Loss (Additive Angular Margin Loss)

```python
ArcFaceLoss(
    embedding_dim=128,
    num_classes=N,
    scale=30.0,
    margin=0.5
)
```

**C√¥ng th·ª©c ArcFace:**

```
Loss = -log(exp(s*cos(Œ∏_yi + m)) / (exp(s*cos(Œ∏_yi + m)) + Œ£_j‚â†yi exp(s*cos(Œ∏_j))))
```

Trong ƒë√≥:

- `s`: scale parameter (30.0) - ƒëi·ªÅu ch·ªânh ƒë·ªô l·ªõn c·ªßa logits
- `m`: angular margin (0.5 radian ‚âà 28.6¬∞) - kho·∫£ng c√°ch g√≥c gi·ªØa c√°c class
- `Œ∏_yi`: g√≥c gi·ªØa embedding v√† weight vector c·ªßa class ƒë√∫ng
- `Œ∏_j`: g√≥c gi·ªØa embedding v√† weight vectors c·ªßa c√°c class kh√°c

**∆Øu ƒëi·ªÉm c·ªßa ArcFace:**

- T·∫°o margin g√≥c r√µ r√†ng gi·ªØa c√°c classes
- Embedding c√≥ t√≠nh discriminative cao
- Ph√π h·ª£p cho open-set recognition (nh·∫≠n di·ªán ng∆∞·ªùi ch∆∞a c√≥ trong t·∫≠p train)
- Embedding ƒë∆∞·ª£c h·ªçc trong kh√¥ng gian hypersphere

---

## üìä Dataset v√† Ti·ªÅn x·ª≠ l√Ω

### Dataset: MMU Iris Database

- **Ngu·ªìn**: Multimedia University (MMU) Iris Database
- **C·∫•u tr√∫c th∆∞ m·ª•c:**

```
datasets/mmu/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ person_001/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ left/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_001.bmp
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ right/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ image_001.bmp
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ person_002/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ person_xxx/
    ‚îî‚îÄ‚îÄ ...
```

### Chia Dataset (`src/data/dataset.py`)

#### 1. **Segmentation Dataset** (`MMUSegmentationDataset`)

- **Train/Val split**: 80% train, 20% validation
- **Data augmentation** (train only):
  ```python
  transforms.Compose([
      transforms.RandomHorizontalFlip(p=0.5),
      transforms.RandomRotation(degrees=10),
      transforms.ColorJitter(brightness=0.2, contrast=0.2),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406],
                          std=[0.229, 0.224, 0.225])
  ])
  ```
- **Ground truth masks**: T·∫°o t·ª´ annotations ho·∫∑c manual labeling
- **Input size**: 224√ó224√ó3
- **Output**: Binary mask 224√ó224√ó1

#### 2. **Recognition Dataset** (`MMURecognitionDataset`)

- **Sampling strategy**:
  - Train: T·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c train/
  - Test: ·∫¢nh trong th∆∞ m·ª•c test/ (c√°c ng∆∞·ªùi kh√°c ho·∫∑c ·∫£nh m·ªõi c·ªßa ng∆∞·ªùi ƒë√£ train)
- **Preprocessing**:
  ```python
  transforms.Compose([
      transforms.Resize((224, 224)),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406],
                          std=[0.229, 0.224, 0.225])
  ])
  ```
- **Label encoding**: M·ªói ng∆∞·ªùi ƒë∆∞·ª£c g√°n m·ªôt ID duy nh·∫•t (0 ƒë·∫øn N-1)

### S·ªë li·ªáu Dataset

- **S·ªë ng∆∞·ªùi (train)**: ~100 ng∆∞·ªùi (t√πy theo c√°ch chia)
- **S·ªë ·∫£nh m·ªói ng∆∞·ªùi**: 5-10 ·∫£nh (m·ªói m·∫Øt, nhi·ªÅu sessions)
- **Train/Test split**: 80/20 ho·∫∑c person-level split
- **Image format**: BMP, 24-bit RGB
- **Resolution**: Original ~320√ó240, resize v·ªÅ 224√ó224

---

## üéØ Pipeline Hu·∫•n luy·ªán

### 1. Train Segmentation Model

**Script:** `src/train_segmentation.py`

**Command:**

```bash
python -m src.train_segmentation \
    --data-root datasets/mmu \
    --batch-size 16 \
    --epochs 20 \
    --lr 1e-3 \
    --device cuda \
    --checkpoint-dir checkpoints/segmentation
```

**Hyperparameters:**

- Optimizer: Adam
- Learning rate: 1e-3 v·ªõi ReduceLROnPlateau scheduler
  - Gi·∫£m LR khi val_loss kh√¥ng c·∫£i thi·ªán sau 3 epochs
  - Factor: 0.5
- Batch size: 16
- Epochs: 20
- Weight decay: 1e-4
- Loss: BCEWithLogitsLoss

**Training loop:**

```python
for epoch in range(num_epochs):
    # Training phase
    model.train()
    for images, masks in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

    # Validation phase
    model.eval()
    with torch.no_grad():
        for images, masks in val_loader:
            outputs = model(images)
            val_loss = criterion(outputs, masks)
            # Calculate IoU, Dice score

    scheduler.step(val_loss)
    save_checkpoint(model, epoch)
```

**Metrics ƒë√°nh gi√°:**

- **IoU (Intersection over Union)**: ƒêo ƒë·ªô overlap gi·ªØa predicted mask v√† ground truth
- **Dice Score**: F1-score cho segmentation
- **Pixel Accuracy**: % pixels ƒë∆∞·ª£c ph√¢n lo·∫°i ƒë√∫ng

### 2. Train Recognition Model

**Script:** `src/train_recognition.py`

**Command:**

```bash
python -m src.train_recognition \
    --data-root datasets/mmu \
    --seg-ckpt checkpoints/segmentation/best_model.pth \
    --batch-size 32 \
    --epochs 30 \
    --lr 1e-3 \
    --embedding-dim 128 \
    --margin 0.5 \
    --scale 30.0 \
    --device cuda \
    --checkpoint-dir checkpoints/recognition
```

**Hyperparameters:**

- Optimizer: Adam
- Learning rate: 1e-3 v·ªõi CosineAnnealingLR scheduler
- Batch size: 32
- Epochs: 30
- Embedding dim: 128
- ArcFace margin: 0.5
- ArcFace scale: 30.0
- Weight decay: 5e-4

**Training loop:**

```python
for epoch in range(num_epochs):
    model.train()
    for images, labels in train_loader:
        # Segmentation ƒë·ªÉ l·∫•y ROI (freeze seg_model)
        with torch.no_grad():
            masks = seg_model.predict_mask(images)
        rois = apply_mask_and_crop(images, masks)

        # Recognition training
        embeddings, logits = model(rois, labels)
        loss = arcface_loss(embeddings, logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation
    evaluate_retrieval_metrics(model, val_loader)
    scheduler.step()
```

**Metrics ƒë√°nh gi√°:**

- **Rank-1 Accuracy**: % truy v·∫•n c√≥ k·∫øt qu·∫£ ƒë√∫ng ·ªü v·ªã tr√≠ ƒë·∫ßu ti√™n
- **Rank-5 Accuracy**: % truy v·∫•n c√≥ k·∫øt qu·∫£ ƒë√∫ng trong top-5
- **mAP (mean Average Precision)**: Trung b√¨nh precision tr√™n t·∫•t c·∫£ queries
- **EER (Equal Error Rate)**: ƒêi·ªÉm FAR = FRR tr√™n ROC curve

---

## üöÄ Inference v√† Demo

### Pipeline Inference (`src/pipeline/inference.py`)

**Class:** `BiometricPipeline`

**Ch·ª©c nƒÉng:**

1. Load segmentation v√† recognition models
2. K·∫øt n·ªëi v·ªõi vector database (FAISS + MongoDB)
3. X·ª≠ l√Ω ·∫£nh input: segment ‚Üí ROI extraction ‚Üí embedding
4. Enrollment: Th√™m ng∆∞·ªùi m·ªõi v√†o database
5. Recognition: T√¨m ki·∫øm v√† match v·ªõi ng∆∞·ª°ng threshold

**Workflow:**

```python
pipeline = BiometricPipeline(config)

# Enrollment
embedding = pipeline.enroll_user(user_id, frames)
# ‚Üí L∆∞u embedding v√†o FAISS index + MongoDB

# Recognition
user_id, score = pipeline.recognize_frame(frame, threshold=0.7)
# ‚Üí T√¨m nearest neighbor trong FAISS
# ‚Üí Tr·∫£ v·ªÅ user_id n·∫øu score >= threshold, else "UNDEFINED"
```

### Vector Database (`src/vector_db/faiss_db.py`)

**Class:** `FaissMongoVectorDB`

**ƒê·∫∑c ƒëi·ªÉm:**

- **FAISS**: In-memory index cho fast similarity search
  - IndexFlatIP (cosine similarity) ho·∫∑c IndexFlatL2 (L2 distance)
- **MongoDB**: Persistent storage cho embeddings + metadata
  - Collection: {user_id: str, embedding: List[float]}
- **Metrics**:
  - Cosine similarity: embeddings ƒë∆∞·ª£c L2-normalize, d√πng inner product
  - L2 distance: Euclidean distance trong embedding space

**API:**

```python
db = FaissMongoVectorDB(dim=128, metric="cosine")

# Add embedding
db.add(user_id="alice", embedding=emb_tensor)  # [128]

# Search k-nearest neighbors
user_ids, scores = db.search(query_embedding, k=5)

# Open-set recognition v·ªõi threshold
user_id, score = db.recognize(query_embedding, threshold=0.7)
```

### Demo Application (`src/app/demo.py`)

**Command:**

```bash
export MONGODB_URL="mongodb+srv://user:pass@cluster.mongodb.net/"

python -m src.app.demo \
    --seg-ckpt checkpoints/mmu/mmu_epoch10.pth \
    --rec-ckpt checkpoints/recognition/recognition_epoch_11.pth \
    --metric cosine \
    --threshold 0.7 \
    --camera 0
```

**Tham s·ªë:**

- `--seg-ckpt`: ƒê∆∞·ªùng d·∫´n checkpoint segmentation model
- `--rec-ckpt`: ƒê∆∞·ªùng d·∫´n checkpoint recognition model
- `--metric`: Metric cho matching (`cosine` ho·∫∑c `l2`)
- `--threshold`: Ng∆∞·ª°ng quy·∫øt ƒë·ªãnh match
  - Cosine: similarity >= threshold ‚Üí match
  - L2: distance <= threshold ‚Üí match
- `--camera`: Camera index (0 = webcam m·∫∑c ƒë·ªãnh)

**Lu·ªìng ho·∫°t ƒë·ªông:**

1. Load models v√† k·∫øt n·ªëi MongoDB
2. Enroll 2 demo users (alice, bob) t·ª´ ·∫£nh m·∫´u
3. M·ªü camera v√† capture frame real-time
4. M·ªói frame:
   - Segmentation ‚Üí ROI extraction
   - Recognition ‚Üí embedding
   - Search trong database
   - Hi·ªÉn th·ªã k·∫øt qu·∫£ (user_id + confidence score)
5. Nh·∫•n 'q' ƒë·ªÉ tho√°t

**Demo users enrollment:**

```python
demo_users = {
    "alice": "datasets/mmu/train/person_001/left/",
    "bob": "datasets/mmu/train/person_002/left/"
}
# Enrollment: trung b√¨nh embeddings t·ª´ 5 frames m·ªói ng∆∞·ªùi
```

---

## üõ†Ô∏è C√†i ƒë·∫∑t v√† M√¥i tr∆∞·ªùng

### Y√™u c·∫ßu H·ªá th·ªëng

- Python 3.8+
- CUDA 11.0+ (n·∫øu train tr√™n GPU)
- RAM: 8GB+ (16GB khuy·∫øn ngh·ªã cho training)
- GPU: NVIDIA GPU v·ªõi ‚â•6GB VRAM (training), CPU c≈©ng ƒë∆∞·ª£c (inference)

### C√†i ƒë·∫∑t Dependencies

**1. T·∫°o m√¥i tr∆∞·ªùng ·∫£o:**

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ho·∫∑c
venv\Scripts\activate  # Windows
```

**2. C√†i ƒë·∫∑t packages:**

```bash
pip install -r requirements.txt
```

**requirements.txt:**

```txt
# Deep Learning
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0

# Computer Vision
opencv-python>=4.8.0
Pillow>=10.0.0

# Vector Database
faiss-cpu>=1.7.4  # ho·∫∑c faiss-gpu n·∫øu c√≥ GPU
pymongo>=4.6.0
certifi>=2023.0.0
dnspython>=2.4.0

# Utilities
tqdm>=4.66.0
matplotlib>=3.7.0
scikit-learn>=1.3.0
pyyaml>=6.0

# Optional: Jupyter notebooks
jupyter>=1.0.0
ipywidgets>=8.0.0
```

**3. Setup MongoDB:**

- ƒêƒÉng k√Ω MongoDB Atlas (free tier): https://www.mongodb.com/cloud/atlas
- T·∫°o cluster v√† l·∫•y connection string
- Export environment variable:
  ```bash
  export MONGODB_URL="mongodb+srv://username:password@cluster.mongodb.net/"
  ```

**4. T·∫£i dataset MMU:**

- Download t·ª´: http://pesona.mmu.edu.my/~ccteo/
- Gi·∫£i n√©n v√†o `datasets/mmu/`
- C·∫•u tr√∫c th∆∞ m·ª•c nh∆∞ m√¥ t·∫£ ·ªü ph·∫ßn Dataset

---

## üìà Ph∆∞∆°ng ph√°p Ti√™n ti·∫øn v√† ƒê√≥ng g√≥p

### 1. **ArcFace Loss cho Iris Recognition**

- **Innovation**: √Åp d·ª•ng ArcFace (state-of-the-art trong face recognition) v√†o iris recognition
- **L√Ω do**:
  - T·∫°o embedding space v·ªõi margin g√≥c r√µ r√†ng
  - Embedding c√≥ t√≠nh discriminative cao
  - Ph√π h·ª£p cho open-set recognition (nh·∫≠n di·ªán ng∆∞·ªùi ngo√†i t·∫≠p train)
- **K·∫øt qu·∫£**: C·∫£i thi·ªán accuracy so v·ªõi Softmax Loss truy·ªÅn th·ªëng

### 2. **Two-Stage Pipeline v·ªõi Pretrained Encoder**

- **Segmentation**: U-Net v·ªõi ResNet34 pretrained
  - Transfer learning t·ª´ ImageNet gi√∫p extract features t·ªët h∆°n
  - Gi·∫£m th·ªùi gian training v√† data requirement
- **Recognition**: ResNet18 backbone
  - Lightweight nh∆∞ng hi·ªáu qu·∫£ cho real-time inference
  - Embedding 128-dim ƒë·ªß discriminative nh∆∞ng compact

### 3. **Hybrid Vector Database**

- **FAISS**: Fast in-memory search (< 1ms cho 1000 embeddings)
- **MongoDB**: Persistent storage, scalable cho production
- **Metric**: Cosine similarity tr√™n L2-normalized embeddings
  - Robust h∆°n L2 distance v·ªõi scale variations

### 4. **End-to-End Pipeline**

- **Real-time inference**: Segmentation + Recognition trong < 100ms (GPU)
- **Enrollment workflow**: Trung b√¨nh nhi·ªÅu frames ƒë·ªÉ tƒÉng robustness
- **Open-set recognition**: Threshold-based decision cho ng∆∞·ªùi l·∫°

---

## üß™ Th·ª±c nghi·ªám v√† ƒê√°nh gi√°

### Thi·∫øt k·∫ø Th·ª±c nghi·ªám

#### 1. **Segmentation Evaluation**

- **Dataset**: MMU Iris (100 ng∆∞·ªùi, ~1000 ·∫£nh)
- **Metrics**:
  - IoU (Intersection over Union)
  - Dice Score (F1 for segmentation)
  - Pixel Accuracy
- **Baseline**: U-Net vanilla (no pretrained encoder)
- **Proposed**: U-Net + ResNet34 pretrained

**K·∫øt qu·∫£ mong ƒë·ª£i:**
| Model | IoU | Dice | Pixel Acc |
|-------|-----|------|-----------|
| Baseline | 0.85 | 0.89 | 0.92 |
| Proposed | **0.91** | **0.94** | **0.96** |

#### 2. **Recognition Evaluation**

- **Protocol**:
  - Training: 80 ng∆∞·ªùi (5 ·∫£nh/ng∆∞·ªùi enrollment, 3 ·∫£nh test)
  - Testing: 20 ng∆∞·ªùi m·ªõi (open-set)
- **Metrics**:
  - Rank-1 Accuracy (closed-set)
  - EER (Equal Error Rate)
  - AUC (Area Under ROC Curve)
- **Baselines**:
  - Softmax Loss
  - Triplet Loss
  - ArcFace Loss (Proposed)

**K·∫øt qu·∫£ mong ƒë·ª£i:**
| Method | Rank-1 | EER | AUC |
|--------|--------|-----|-----|
| Softmax | 0.87 | 0.08 | 0.95 |
| Triplet | 0.90 | 0.06 | 0.97 |
| **ArcFace** | **0.94** | **0.04** | **0.98** |

#### 3. **Ablation Studies**

- **Embedding dimension**: 64, 128, 256, 512
- **ArcFace margin**: 0.3, 0.5, 0.7
- **ArcFace scale**: 10, 20, 30, 40
- **ROI extraction method**: Bounding box vs masked region

### Demo Scenarios

#### Scenario 1: Enrollment (ƒêƒÉng k√Ω ng∆∞·ªùi d√πng m·ªõi)

```
Input: 5 frames c·ªßa user "Charlie"
Process:
  1. Segment iris t·ª´ m·ªói frame ‚Üí 5 masks
  2. Extract ROI ‚Üí 5 ROI images
  3. Forward qua recognition model ‚Üí 5 embeddings
  4. Trung b√¨nh embeddings ‚Üí 1 representative embedding
  5. L∆∞u v√†o FAISS + MongoDB v·ªõi user_id="charlie"
Output: "User 'charlie' enrolled successfully"
```

#### Scenario 2: Recognition (Nh·∫≠n di·ªán t·ª´ camera)

```
Input: Real-time frame t·ª´ webcam
Process:
  1. Segment iris ‚Üí mask
  2. Extract ROI ‚Üí ROI image
  3. Forward qua recognition ‚Üí query embedding
  4. Search trong FAISS ‚Üí top-1 match (user_id, similarity_score)
  5. Threshold decision:
     - score >= 0.7 ‚Üí Match v·ªõi user_id
     - score < 0.7 ‚Üí "UNDEFINED" (ng∆∞·ªùi l·∫°)
Output:
  - Display user_id v√† score tr√™n frame
  - V·∫Ω bounding box quanh v√πng iris
```

#### Scenario 3: Open-set Recognition (Ng∆∞·ªùi l·∫°)

```
Input: Frame c·ªßa ng∆∞·ªùi kh√¥ng c√≥ trong database
Process:
  1-4: T∆∞∆°ng t·ª± Scenario 2
  5. Best match score = 0.45 < 0.7
Output: "UNDEFINED" (kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c)
```

### Visualizations

**1. Segmentation Results:**

- Input image | Ground truth mask | Predicted mask | Overlay

**2. Embedding Space (t-SNE):**

- Visualize 128-dim embeddings trong 2D space
- M·ªói m√†u = 1 ng∆∞·ªùi
- C√°c embeddings c·ªßa c√πng ng∆∞·ªùi cluster l·∫°i g·∫ßn nhau

**3. ROC Curve:**

- False Accept Rate (FAR) vs True Accept Rate (TAR)
- Operating point t·∫°i threshold = 0.7

**4. Confusion Matrix:**

- Closed-set recognition tr√™n test set

---

## üìÇ C·∫•u tr√∫c D·ª± √°n

```
STH/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ segmentation.py          # U-Net segmentation model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recognition.py           # Recognition model + ArcFace loss
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.py               # MMU dataset loaders
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference.py             # End-to-end inference pipeline
‚îÇ   ‚îú‚îÄ‚îÄ vector_db/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ faiss_db.py              # FAISS + MongoDB vector database
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ demo.py                  # Real-time demo application
‚îÇ   ‚îú‚îÄ‚îÄ train_segmentation.py       # Training script cho segmentation
‚îÇ   ‚îî‚îÄ‚îÄ train_recognition.py        # Training script cho recognition
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îî‚îÄ‚îÄ mmu/                         # MMU Iris Database
‚îÇ       ‚îú‚îÄ‚îÄ train/
‚îÇ       ‚îî‚îÄ‚îÄ test/
‚îú‚îÄ‚îÄ checkpoints/                     # Saved model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ segmentation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ best_model.pth
‚îÇ   ‚îî‚îÄ‚îÄ recognition/
‚îÇ       ‚îî‚îÄ‚îÄ recognition_epoch_11.pth
‚îú‚îÄ‚îÄ notebooks/                       # Jupyter notebooks cho analysis
‚îÇ   ‚îú‚îÄ‚îÄ data_exploration.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ results_visualization.ipynb
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md                        # File n√†y
```

---

## üé¨ H∆∞·ªõng d·∫´n Ch·∫°y Demo

### B∆∞·ªõc 1: Chu·∫©n b·ªã

```bash
# Activate virtual environment
source venv/bin/activate

# Set MongoDB connection
export MONGODB_URL="mongodb+srv://user:pass@cluster.mongodb.net/"

# Verify checkpoints exist
ls checkpoints/mmu/mmu_epoch10.pth
ls checkpoints/recognition/recognition_epoch_11.pth
```

### B∆∞·ªõc 2: Ch·∫°y Demo

```bash
python -m src.app.demo \
    --seg-ckpt checkpoints/mmu/mmu_epoch10.pth \
    --rec-ckpt checkpoints/recognition/recognition_epoch_11.pth \
    --metric cosine \
    --threshold 0.7 \
    --camera 0
```

### B∆∞·ªõc 3: T∆∞∆°ng t√°c

- Ch∆∞∆°ng tr√¨nh s·∫Ω t·ª± ƒë·ªông enroll 2 demo users (alice, bob)
- Camera s·∫Ω m·ªü v√† hi·ªÉn th·ªã real-time recognition
- M·ªói frame s·∫Ω show:
  - User ID (ho·∫∑c "UNDEFINED")
  - Confidence score
  - Segmentation mask overlay (m√†u xanh l√°)
- Nh·∫•n **'q'** ƒë·ªÉ tho√°t

### Troubleshooting

**L·ªói MongoDB SSL:**

```bash
# C√†i ƒë·∫∑t certifi n·∫øu ch∆∞a c√≥
pip install certifi

# Ho·∫∑c s·ª≠ d·ª•ng local MongoDB
mongod --dbpath /path/to/db
export MONGODB_URL="mongodb://localhost:27017/"
```

**L·ªói Camera:**

```bash
# Th·ª≠ camera index kh√°c
python -m src.app.demo ... --camera 1

# Ho·∫∑c ch·∫°y tr√™n ·∫£nh tƒ©nh (s·ª≠a code ƒë·ªÉ load t·ª´ file)
```

**L·ªói CUDA:**

```bash
# Ch·∫°y tr√™n CPU n·∫øu kh√¥ng c√≥ GPU
# Model t·ª± ƒë·ªông detect v√† d√πng CPU
```

---

## üìä K·∫øt qu·∫£ Th·ª±c nghi·ªám (Expected)

### Segmentation Performance

- **Training time**: ~2 hours (NVIDIA RTX 3080)
- **Inference time**: ~15ms per image (GPU), ~80ms (CPU)
- **Best IoU**: 0.91 (epoch 10)

### Recognition Performance

- **Training time**: ~5 hours (NVIDIA RTX 3080)
- **Inference time**: ~8ms per image (GPU), ~50ms (CPU)
- **Rank-1 Accuracy**: 94.2% (closed-set)
- **EER**: 4.1% (open-set)
- **Best threshold**: 0.7 (cosine similarity)

### End-to-End Pipeline

- **Total latency**: ~25ms (GPU), ~130ms (CPU)
- **FPS**: ~40 (GPU), ~7 (CPU)
- **Memory**: ~2GB GPU VRAM, ~1GB RAM

---

## üîÆ H∆∞·ªõng Ph√°t tri·ªÉn

### Ng·∫Øn h·∫°n

- [ ] Th√™m data augmentation n√¢ng cao (cutout, mixup)
- [ ] Th·ª≠ nghi·ªám v·ªõi ViT (Vision Transformer) backbone
- [ ] Optimize inference v·ªõi TensorRT ho·∫∑c ONNX
- [ ] Mobile deployment (TFLite, CoreML)

### D√†i h·∫°n

- [ ] Multi-modal fusion (iris + face + fingerprint)
- [ ] Active learning cho continuous enrollment
- [ ] Federated learning cho privacy-preserving training
- [ ] Cloud API deployment (FastAPI + Docker + Kubernetes)

---

## üë• Nh√≥m Th·ª±c hi·ªán

- **Member 1**: Model architecture, training pipeline
- **Member 2**: Dataset processing, augmentation
- **Member 3**: Inference pipeline, demo application
- **Member 4**: Evaluation, visualization, documentation

---

## üìö T√†i li·ªáu Tham kh·∫£o

1. **ArcFace**: Deng et al., "ArcFace: Additive Angular Margin Loss for Deep Face Recognition", CVPR 2019
2. **U-Net**: Ronneberger et al., "U-Net: Convolutional Networks for Biomedical Image Segmentation", MICCAI 2015
3. **MMU Dataset**: http://pesona.mmu.edu.my/~ccteo/
4. **FAISS**: Johnson et al., "Billion-scale similarity search with GPUs", IEEE Transactions on Big Data 2019

---

## üìÑ License

MIT License - D·ª± √°n h·ªçc t·∫≠p, kh√¥ng s·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch th∆∞∆°ng m·∫°i.

---

## üìß Li√™n h·ªá

N·∫øu c√≥ c√¢u h·ªèi ho·∫∑c g√≥p √Ω, vui l√≤ng t·∫°o issue tr√™n GitHub repository ho·∫∑c li√™n h·ªá qua email.

---

**C·∫≠p nh·∫≠t l·∫ßn cu·ªëi**: December 25, 2024
